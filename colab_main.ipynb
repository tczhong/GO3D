{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.0 64-bit ('dl': conda)"
  },
  "interpreter": {
   "hash": "6634cf679a7e831245905c16a3fb12b9728447c4009b402b2ce7755e57ba4d30"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-1b18610b05d0>, line 1)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-1b18610b05d0>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    args.NUM_POINTS =\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "args.NUM_CLASSES = 10\n",
    "args.BATCH_SIZE = 32\n",
    "args.EPOCHS = 30\n",
    "args.AUGMENT = True\n",
    "args.MODEL = 'pointnet'\n",
    "args.LEARNING_RATE = 0.001\n",
    "args.DROPOUT_RATE = 0.3\n",
    "args.PRINT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_name = './data/parsed_data_' + str(args.NUM_POINTS) + '.pkl'\n",
    "# data_file_name = './data/parsed_data.pkl'\n",
    "train_points = None\n",
    "test_points = None\n",
    "train_labels = None\n",
    "test_labels = None\n",
    "class_map = None\n",
    "\n",
    "if not os.path.isfile(data_file_name):\n",
    "    data.save_data(args.NUM_POINTS)\n",
    "\n",
    "with open(data_file_name, 'rb') as infile:\n",
    "    train_points, test_points, train_labels, test_labels, class_map = pkl.load(infile)\n",
    "    print('train_points:', train_points.shape)\n",
    "    print('test_points:', test_points.shape)\n",
    "    print('train_labels:', train_labels.shape)\n",
    "    print('test_labels:', test_labels.shape)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_points, train_labels))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_points, test_labels))\n",
    "\n",
    "\n",
    "if args.AUGMENT == True:\n",
    "    train_dataset = train_dataset.shuffle(len(train_points)).map(augment).batch(args.BATCH_SIZE)\n",
    "else:\n",
    "    train_dataset = train_dataset.shuffle(len(train_points)).batch(args.BATCH_SIZE)\n",
    "test_dataset = test_dataset.shuffle(len(test_points)).batch(args.BATCH_SIZE)\n",
    "\n",
    "time_stamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "output_dir = './outputs/'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "log_dir = output_dir + args.MODEL + '_' + time_stamp + '/'\n",
    "os.mkdir(log_dir)\n",
    "print('MODEL LOGGGER:', log_dir)\n",
    "\n",
    "# Construct Model\n",
    "model_structure = model.model_build(NUM_POINTS=args.NUM_POINTS, NUM_CLASSES=args.NUM_CLASSES,\n",
    "                                    DROPOUT_RATE=args.DROPOUT_RATE, PRINT=args.PRINT)\n",
    "network = model_structure.load(MODEL=args.MODEL, log_dir=log_dir)\n",
    "\n",
    "network.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=args.LEARNING_RATE),\n",
    "    metrics=[\"sparse_categorical_accuracy\"],\n",
    ")\n",
    "\n",
    "model_callbacks = [\n",
    "    keras.callbacks.TensorBoard(log_dir=log_dir),\n",
    "    #keras.callbacks.ModelCheckpoint(filepath=args.MODEL+'_{epoch:02d}.h5'),\n",
    "    keras.callbacks.CSVLogger(filename=log_dir+'model_training.log')\n",
    "]\n",
    "\n",
    "network.fit(train_dataset, epochs=args.EPOCHS, validation_data=test_dataset,\n",
    "            callbacks=model_callbacks)\n"
   ]
  }
 ]
}